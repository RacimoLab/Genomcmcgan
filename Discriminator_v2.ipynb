{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Discriminator_v2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPICPIaxzuvGns6J24vje27",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pabloswfly/mcmcgan/blob/master/Discriminator_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJrCbN0cjl90",
        "colab_type": "text"
      },
      "source": [
        "Now that we have learn to do demographic simulations with msprime, we can start building our discriminator with the permutation invariance property. The discriminator in the beginning will tell apart simulations with a difference in a single parameter, such as mutation rate, recombination rate, effective size, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8MM935TkE5j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Installing required libraries\n",
        "!apt-get install python-dev libgsl0-dev\n",
        "!python3 -m pip install msprime stdpopsim\n",
        "!pip install zarr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHv7M0ScGM6n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing libraries and modules\n",
        "import msprime, stdpopsim, zarr, random, imageio, bisect\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Mount my Gdrive disk\n",
        "from google.colab import drive, files\n",
        "drive.mount('/content/gdrive')\n",
        "!mkdir /content/results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYN4Y2SYjjDp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fixed_size_geno(ts, x):\n",
        "  \"\"\"Returns a genotype matrix with a fixed number of columns, as specified in x\"\"\"\n",
        "  \n",
        "  m = np.zeros((ts.num_samples, x), dtype=float)\n",
        "\n",
        "  for variant in ts.variants():\n",
        "      j = int(variant.site.position * x / ts.sequence_length)\n",
        "      m[:, j] += variant.genotypes\n",
        "\n",
        "  return m\n",
        "\n",
        "\n",
        "\n",
        "def scale_matrix(m):\n",
        "  \"\"\"Scale matrix values to [-1, 1] range\"\"\"\n",
        "  return (m*2/np.max(m) - 1)\n",
        "\n",
        "\n",
        "\n",
        "def draw_genmat(img, name):\n",
        "  \n",
        "  plt.imshow(img, cmap=\"winter\")\n",
        "  plt.title(f'genomat_{name}')\n",
        "  plt.savefig(f'genomat_{name}.png')\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "\n",
        "def simulate_msprime(samples, seqlen, n_reps, fixed_size, param, seed=None, scale=True, \n",
        "                     param_name='recomb'):\n",
        "  \"\"\"Simulate demographic data, returning a tensor with n_reps number of genotype matrices\"\"\"\n",
        "\n",
        "  if param_name=='recomb':\n",
        "    sims = msprime.simulate(sample_size=samples, Ne=10000, length=seqlen, mutation_rate=1e-8, \n",
        "              recombination_rate=param, num_replicates=n_reps, random_seed=seed)\n",
        "    \n",
        "  elif param_name=='mutation':\n",
        "    sims = msprime.simulate(sample_size=samples, Ne=10000, length=seqlen, mutation_rate=param, \n",
        "              recombination_rate=1e-8, num_replicates=n_reps, random_seed=seed)\n",
        "    \n",
        "  elif param_name=='effective':\n",
        "    sims = msprime.simulate(sample_size=samples, Ne=param, length=seqlen, mutation_rate=1e-8, \n",
        "              recombination_rate=1e-8, num_replicates=n_reps, random_seed=seed)\n",
        "    \n",
        "  elif param_name=='growth':\n",
        "    popconfig = msprime.PopulationConfiguration(sample_size=samples, growth_rate=param)\n",
        "    sims = msprime.simulate(Ne=10000, length=seqlen, mutation_rate=1e-8, recombination_rate=1e-8, \n",
        "                            num_replicates=n_reps, random_seed=seed, \n",
        "                            population_configurations=popconfig)\n",
        "\n",
        "  mat = np.zeros((n_reps, samples, fixed_size))\n",
        "\n",
        "  # For each tree sequence output from the simulation\n",
        "  for i, treeseq in enumerate(sims):\n",
        "    mat[i] = fixed_size_geno(treeseq, fixed_size)\n",
        "\n",
        "  # Scale genotype matrices from [0, 1] to [-1, 1].\n",
        "  # If we were to use a generator, this scale should be done with tanh function\n",
        "  if scale:\n",
        "    mat = scale_matrix(mat)\n",
        "\n",
        "  # Expand dimension by 1 (add channel dim). -1 stands for last axis.\n",
        "  mat = np.expand_dims(mat, axis=-1)\n",
        "\n",
        "  return mat\n",
        "\n",
        "\n",
        "\n",
        "def simulate_msprime_list(samples, seqlen, fixed_size, params, seed=None, scale=True, \n",
        "                          param_name='recomb'):\n",
        "  \"\"\"Simulate demographic data, returning a tensor with n_reps number of \n",
        "  genotype matrices. Recombination rate is a list of values, not a single scalar\"\"\"\n",
        "\n",
        "  sims = []\n",
        "  n_reps = len(params)\n",
        "\n",
        "  if param_name=='recomb':\n",
        "    for p in params:\n",
        "      sims.append(msprime.simulate(sample_size=samples, Ne=10000, length=seqlen, \n",
        "                                   mutation_rate=1e-8, recombination_rate=p, random_seed=seed))\n",
        "    \n",
        "  elif param_name=='mutation':\n",
        "    for p in params:\n",
        "      sims.append(msprime.simulate(sample_size=samples, Ne=10000, length=seqlen, mutation_rate=p, \n",
        "                                   recombination_rate=1e-8, random_seed=seed))\n",
        "  \n",
        "  elif param_name=='effective':\n",
        "    for p in params:\n",
        "      sims.append(msprime.simulate(sample_size=samples, Ne=p, length=seqlen, mutation_rate=1e-8, \n",
        "                                   recombination_rate=1e-8, random_seed=seed))\n",
        "      \n",
        "  elif param_name=='growth':\n",
        "    for p in params:\n",
        "      popconfig = msprime.PopulationConfiguration(sample_size=samples, growth_rate=p)\n",
        "      sims = msprime.simulate(Ne=10000, length=seqlen, mutation_rate=1e-8, recombination_rate=1e-8, \n",
        "                      num_replicates=n_reps, random_seed=seed, population_configurations=popconfig)\n",
        "\n",
        "  mat = np.zeros((n_reps, samples, fixed_size))\n",
        "\n",
        "  # For each tree sequence output from the simulation\n",
        "  for i, treeseq in enumerate(sims):\n",
        "    mat[i] = fixed_size_geno(treeseq, fixed_size)\n",
        "\n",
        "  # Scale genotype matrices from [0, 1] to [-1, 1].\n",
        "  # If we were to use a generator, this scale should be done with tanh function\n",
        "  if scale:\n",
        "    mat = scale_matrix(mat)\n",
        "\n",
        "  # Expand dimension by 1 (add channel dim). -1 stands for last axis.\n",
        "  mat = np.expand_dims(mat, axis=-1)\n",
        "\n",
        "  return mat\n",
        "\n",
        "\n",
        "\n",
        "def generate_realdata(samples, seqlen, n_reps, fixed_size, scale=True):\n",
        "\n",
        "  # Set up some data paths\n",
        "  mask_file = \"/content/gdrive/My Drive/mcmcgan/20140520.pilot_mask.autosomes.bed\"\n",
        "  zarr_path = \"/content/gdrive/My Drive/mcmcgan/zarr\"\n",
        "\n",
        "  # Localte the data contained in zarr\n",
        "  callset = zarr.open_group(zarr_path, mode='r')\n",
        "  n_samples = len(callset['1/samples'])\n",
        "\n",
        "  real_data = np.zeros((n_reps, n_samples, fixed_size))\n",
        "\n",
        "  # Get lists of randomly selected chromosomes and genomic locations\n",
        "  chroms, pos, slices = random_sampling_geno(callset, seqlen, n_reps, mask_file=mask_file)\n",
        "\n",
        "  # For each randomly sampled genomic location\n",
        "  for i, (chrom, pos, loc_region) in enumerate(zip(chroms, pos, slices)):\n",
        "    print(f'it {i}  :  chromosome {chrom}  :  position {pos}')\n",
        "\n",
        "    # Extract genotype and genomic position for the variants for all samples\n",
        "    gt_zarr = np.asarray(callset[f'{chrom}/calldata/GT'][loc_region])\n",
        "    pos_zarr = callset[f'{chrom}/variants/POS'][loc_region]\n",
        "\n",
        "    # Make sure the genome is diploid, and extract one of the haplotypes\n",
        "    assert gt_zarr.shape[2] is 2, \"Samples are not diploid\"\n",
        "    hap = haploidify(gt_zarr, h=0)\n",
        "\n",
        "    # Get the relative position in the sequence length to resize the matrix\n",
        "    relative_pos = pos_zarr - pos\n",
        "    real_data[i] = resize(hap, relative_pos, fixed_size, seqlen)\n",
        "\n",
        "  if scale:\n",
        "    real_data = scale_matrix(real_data)\n",
        "\n",
        "  real_data = np.expand_dims(real_data, axis=-1)\n",
        "  \n",
        "  return real_data\n",
        "\n",
        "\n",
        "\n",
        "def generate_data(samples, seqlen, n_reps, genmat_cols, param_name, fixed, low, high, \n",
        "                  minuslog=True, scale=True, source='msprime'):\n",
        "  #Generate (X, y) data from demographic simulations.\n",
        "\n",
        "  print(f'generating {n_reps} genotype matrices from {source}')\n",
        "  if source=='stdpopsim':\n",
        "    gen0 = simulate_stdpopsim(engine='msprime', species='HomSap', \n",
        "                        model='OutOfAfricaArchaicAdmixture_5R19', pop='CEU', samples=samples, \n",
        "                        seqlen=seqlen, reps=n_reps, scale=scale, genmat_cols=genmat_cols, \n",
        "                        error_prob=None)\n",
        "    \n",
        "  elif source=='real data':\n",
        "    gen0 = generate_realdata(samples, seqlen, n_reps, genmat_cols)\n",
        "\n",
        "  elif source=='msprime':\n",
        "    gen0 = simulate_msprime(samples, seqlen, n_reps, genmat_cols, fixed, None, scale, param_name)\n",
        "\n",
        "  print(f'generating {n_reps} genotype matrices with different {param_name} from msprime')\n",
        "  exps = -np.linspace(low, high, num=n_reps) if minuslog else np.linspace(low, high, num=n_reps)\n",
        "  paramlist = np.power(10, exps)\n",
        "  gen1 = simulate_msprime_list(samples, seqlen, genmat_cols, paramlist, None, scale, param_name)\n",
        "\n",
        "  X = np.concatenate((gen0, gen1))\n",
        "  y = np.concatenate((np.zeros((n_reps)), np.ones((n_reps))))\n",
        "  print(f'X data shape is: {X.shape}')\n",
        "\n",
        "  draw_genmat(np.squeeze(gen0[0]), name=f'{param_name}_{fixed}')\n",
        "  draw_genmat(np.squeeze(gen1[0]), name=f'{param_name}_{high}:{low}')\n",
        "\n",
        "  #Split randomly into training and test data.\n",
        "  return train_test_split(X, y, test_size=0.1)\n",
        "  \n",
        "\n",
        "\n",
        "def simulate_stdpopsim(engine, species, model, pop, samples, seqlen, reps, genmat_cols, \n",
        "                       scale=True, error_prob=None, seed=None):\n",
        "\n",
        "  stdengine = stdpopsim.get_engine(engine)\n",
        "  stdspecies = stdpopsim.get_species(species)\n",
        "  stdmodel = stdspecies.get_demographic_model(model)\n",
        "\n",
        "  geno = [(i, get_chrom_size(i)) for i in range(1, 23)]\n",
        "  # Sort the list by size.\n",
        "  geno.sort(key=lambda a: a[1], reverse=True)\n",
        "  cum_weights = []\n",
        "  rng = random.Random(seed)\n",
        "  for i, (chrom, size) in enumerate(geno):\n",
        "      cum_weights.append(size if i == 0 else size + cum_weights[i-1])\n",
        "\n",
        "  # The order for sampling from populations is ['YRI', 'CEU', 'CHB']\n",
        "  if pop=='CEU':\n",
        "    stdsamples = stdmodel.get_samples(0, samples, 0) \n",
        "\n",
        "  sims = []\n",
        "  for i in range(n_reps):\n",
        "    chrom, size = rng.choices(geno, cum_weights=cum_weights)[0]\n",
        "    factor = seqlen/size\n",
        "    stdcontig = stdspecies.get_contig('chr' + str(chrom), length_multiplier=factor)\n",
        "    sims.append(stdengine.simulate(stdmodel, stdcontig, stdsamples))\n",
        "\n",
        "  mat = np.zeros((reps, samples, genmat_cols))\n",
        "\n",
        "  # For each tree sequence output from the simulation\n",
        "  for i, treeseq in enumerate(sims):\n",
        "\n",
        "    if error_prob==None:\n",
        "      mat[i] = fixed_size_geno(treeseq, genmat_cols)\n",
        "\n",
        "    elif type(error_prob) is list:\n",
        "      mat[i] = mutate_geno(treeseq, genmat_cols, p=error_prob[i])\n",
        "\n",
        "    else:\n",
        "      mat[i] = mutate_geno(treeseq, genmat_cols, p=error_prob)\n",
        "\n",
        "  # Scale genotype matrices from [0, 1] to [-1, 1].\n",
        "  # If we were to use a generator, this scale should be done with tanh function\n",
        "  if scale:\n",
        "    mat = scale_matrix(mat)\n",
        "\n",
        "  # Expand dimension by 1 (add channel dim). -1 stands for last axis.\n",
        "  mat = np.expand_dims(mat, axis=-1)\n",
        "\n",
        "  return mat\n",
        "\n",
        "\n",
        "\n",
        "def mutate_geno(ts, x, p=0.001):\n",
        "  \"\"\"Returns a genotype matrix with a fixed number of columns, as specified in x\"\"\"\n",
        "\n",
        "  rows = ts.num_samples\n",
        "  cols = int(ts.sequence_length)\n",
        "  m = np.zeros((rows, cols), dtype=float)\n",
        "\n",
        "  for variant in ts.variants():\n",
        "      m[:, int(variant.site.position)] += variant.genotypes\n",
        "\n",
        "  m = m.flatten()\n",
        "  n = np.random.binomial(len(m), p)\n",
        "  idx = np.random.randint(0, len(m), size=n)\n",
        "  m[idx] = 1 - m[idx]\n",
        "  m = m.reshape((rows, cols))\n",
        "\n",
        "  f = int(cols/x)\n",
        "  mat = np.zeros((rows, x), dtype=float)\n",
        "\n",
        "  for i in range(x):\n",
        "    s = i*f\n",
        "    e = s + f - 1\n",
        "    mat[:, i] = np.sum(m[:, s:e], axis=1)\n",
        "\n",
        "  return mat\n",
        "\n",
        "\n",
        "\n",
        "def get_chrom_size(chrom):\n",
        "  \"\"\"These sizes are based on the catalog for Homosapiens in stdpopsim, but they're exactly the\n",
        "  same as the one given by the VCF files, so I use them for both real and simulated data\"\"\"\n",
        "\n",
        "  chrom = str(chrom)\n",
        "  lengths = {'1': 249250621, '2': 243199373, '3': 198022430, '4': 191154276, '5': 180915260, \n",
        "             '6': 171115067, '7': 159138663, '8': 146364022, '9': 141213431, '10': 135534747, \n",
        "             '11': 135006516, '12': 133851895, '13': 115169878, '14':107349540, '15': 102531392, \n",
        "             '16': 90354753, '17': 81195210, '18': 78077248,'19': 59128983, '20': 63025520, \n",
        "             '21': 48129895, '22': 51304566}\n",
        "\n",
        "  return lengths[chrom]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def load_mask(mask_file, min_len):\n",
        "  \"\"\"Given a mask file in BED format, parse the mask data and returns a matrix of\n",
        "      tuples containing the permited regions, as (start, end) positions\"\"\"\n",
        "\n",
        "  # Initialize empty mask dictionary\n",
        "  mask = {str(k): [] for k in range(1, 23)}\n",
        "\n",
        "  # Read through the lines and add to the dictionary for each chrom\n",
        "  with open(mask_file, 'r') as file:\n",
        "    for line in file:\n",
        "      chrom, start, end, _ = line.split()\n",
        "      start, end = int(start), int(end)\n",
        "\n",
        "      if (end - start) > min_len:\n",
        "        mask[chrom[3:]].append((int(start), int(end)))\n",
        "    \n",
        "    file.close()\n",
        "  \n",
        "  return mask\n",
        "\n",
        "\n",
        "\n",
        "def random_sampling_geno(callset, seqlen, n_reps, mask_file = None, seed = None):\n",
        "  \"\"\"random sampling from chromosome based on the proportional size and the mask\"\"\"\n",
        "\n",
        "  # Extract chromosome number and length from stdpopsim catalog\n",
        "  geno = [(i, get_chrom_size(i)) for i in range(1, 23)]\n",
        "\n",
        "  # Sort the list by size.\n",
        "  geno.sort(key=lambda a: a[1], reverse=True)\n",
        "\n",
        "  cum_weights = []\n",
        "  for i, (chrom, size) in enumerate(geno):\n",
        "      cum_weights.append(size if i == 0 else size + cum_weights[i-1])\n",
        "\n",
        "  print(\"Charging up the chromosomes\")\n",
        "  locs = [0]\n",
        "  for i in range(1, 23):\n",
        "    print(f\"Charging chromosome {i}\")\n",
        "    query = f\"{i}/variants/POS\"\n",
        "    locs.append(np.asarray(callset[query]))\n",
        "  \n",
        "  mask = load_mask(mask_file, min_len = 10000) if mask_file else None\n",
        "\n",
        "  rng = random.Random(seed)\n",
        "  chroms, slices, mask_ranges, pos = [], [], [], []\n",
        "\n",
        "  while len(chroms) < n_reps:\n",
        "    chrom, size = rng.choices(geno, cum_weights=cum_weights)[0]\n",
        "\n",
        "    assert size > seqlen\n",
        "    proposal = rng.randrange(0, size - seqlen)\n",
        "\n",
        "    if mask:\n",
        "      for start, end in mask[str(chrom)]:\n",
        "        if start < proposal < end:\n",
        "          chroms.append(chrom)\n",
        "          pos.append(proposal)\n",
        "          slices.append(locate(locs[chrom], start=proposal, stop=proposal + seqlen))\n",
        "\n",
        "    else:\n",
        "      chroms.append(chrom)\n",
        "      pos.append(proposal)\n",
        "      slices.append(locate(locs[chrom], start=proposal, stop=proposal + seqlen))\n",
        "            \n",
        "  return chroms, pos, slices\n",
        "\n",
        "\n",
        "\n",
        "def locate(sorted_idx, start=None, stop=None):\n",
        "  \"\"\"This implementation comes from scikit-allel library. Change it a little for copyright lol\"\"\"\n",
        "  \n",
        "  start_idx = bisect.bisect_left(sorted_idx, start) if start is not None else 0\n",
        "  stop_idx = bisect.bisect_right(sorted_idx, stop) if stop is not None else len(v)\n",
        "\n",
        "  return slice(start_idx, stop_idx)\n",
        "\n",
        "\n",
        "\n",
        "def haploidify(genmat, h):\n",
        "  \"\"\"Returns the selected haplotype from a numpy array with a ploidy dimension.\n",
        "      The parameter h must be either 0 or 1. TODO: Possibility of selecting both\"\"\"\n",
        "  return genmat[:, :, h]\n",
        "\n",
        "\n",
        "\n",
        "def resize(mat, pos, fixed_size, seq_len):\n",
        "  \"\"\"Resizes a matrix using a sum window, given a genotype matrix, positions vector,\n",
        "     sequence length and the desired fixed size of the new matrix\"\"\"\n",
        "\n",
        "  # Initialize empty matrix with the new dimensions\n",
        "  m = np.zeros((fixed_size, mat.shape[1]), dtype=mat.dtype)\n",
        "\n",
        "  # Fill in the resized matrix\n",
        "  for _pos, _gt in zip(pos, mat):\n",
        "    j = int(_pos * fixed_size / seq_len) - 1\n",
        "    np.add(m[j, :], _gt, out=m[j, :], where=_gt != -1)\n",
        "\n",
        "  return m.T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2-3pDVymG5C",
        "colab_type": "text"
      },
      "source": [
        "The genotype data X contains the segregating sites (SNPs) from the two simulated scenarios, which are labelled accordingly in y.  For each independent simulation, the genotype matrix has shape (samples, matrix_cols). X contains 3000 data entries, 1500 from each class, and divided into 2.400 training data and 600 testing data. The labels are 0 for recombination rate of 1e-8 and 1 for 1e-9.\n",
        "\n",
        "We are going to visualize a genotype matrix from each different scenario as an image with matplotlib:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpr-iMZ7RTmf",
        "colab_type": "text"
      },
      "source": [
        "Now we will build the discriminator with the permutation invariance property. The Symmetric layer class is a type of layer that applies a summary statistics throughout the selected axis in the input tensor, and collapses that axis dimension to 1 in the output tensor. Different summary statistics can be chosen, such as 'max', 'min', 'sum' and 'mean'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gdZaoL8R7fg",
        "colab_type": "text"
      },
      "source": [
        "In order to allow permutation invariance, the convolutional kernels have a dimension of 1xN (where N=6 at the moment). Also, instead of using pooling layers for downsampling, strides of (1,2) are used, so the number of different rows/haplotypes remains intact, but the sequence length is halved at that Conv2d layer.\n",
        "Following successful GANs tips, the network also includes Dropout, Batch Normalization and LeakyReLU activation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JiEhKjRjusBD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Symmetric(keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, function, axis, **kwargs):\n",
        "      self.function = function\n",
        "      self.axis = axis\n",
        "      super(Symmetric, self).__init__(**kwargs)\n",
        "\n",
        "    def call(self, x):\n",
        "      if self.function == 'sum':\n",
        "        out = keras.backend.sum(x, axis=self.axis, keepdims=True)\n",
        "      if self.function == 'mean':\n",
        "        out = keras.backend.mean(x, axis=self.axis, keepdims=True)\n",
        "      if self.function == 'min':\n",
        "        out = keras.backend.min(x, axis=self.axis, keepdims=True)\n",
        "      if self.function == 'max':\n",
        "        out = keras.backend.max(x, axis=self.axis, keepdims=True)\n",
        "      return out\n",
        "\n",
        "\n",
        "\n",
        "def build_discriminator(model, in_shape):\n",
        "  \"\"\"Build different Convnet models with permutation variance property\"\"\"\n",
        "\n",
        "  cnn = keras.models.Sequential(name='discriminator')\n",
        "\n",
        "  if model==1:\n",
        "    \"\"\"The slowest model, but good accuracy. 31 s/epoch\"\"\"\n",
        "\n",
        "    # None in input_shape for dimensions with variable size.\n",
        "    cnn.add(keras.layers.Conv2D(filters=16, kernel_size=(1, 9), padding='same', \n",
        "                                strides=(1, 1), input_shape=in_shape))\n",
        "    cnn.add(keras.layers.LeakyReLU(0.3))\n",
        "    cnn.add(keras.layers.BatchNormalization())\n",
        "\n",
        "    cnn.add(keras.layers.Conv2D(filters=16, kernel_size=(1, 9), padding='same', strides=(1, 2)))\n",
        "    cnn.add(keras.layers.LeakyReLU(0.3))\n",
        "    cnn.add(keras.layers.BatchNormalization())\n",
        "    cnn.add(keras.layers.Dropout(0.3))\n",
        "\n",
        "    cnn.add(Symmetric('mean', axis=1))\n",
        "\n",
        "    cnn.add(keras.layers.Conv2D(filters=32, kernel_size=(1, 9), padding='same', strides=(1, 2)))\n",
        "    cnn.add(keras.layers.LeakyReLU(0.3))\n",
        "    cnn.add(keras.layers.BatchNormalization())\n",
        "    cnn.add(keras.layers.Dropout(0.3))\n",
        "\n",
        "    cnn.add(Symmetric('mean', axis=2))\n",
        "\n",
        "\n",
        "  elif model==2:\n",
        "    \"\"\"23 s/epoch\"\"\"\n",
        "\n",
        "    # None in input_shape for dimensions with variable size.\n",
        "    cnn.add(keras.layers.Conv2D(filters=8, kernel_size=(1, 9), padding='same', \n",
        "                                strides=(1, 1), input_shape=in_shape))\n",
        "    cnn.add(keras.layers.LeakyReLU(0.3))\n",
        "    cnn.add(keras.layers.BatchNormalization())\n",
        "\n",
        "    cnn.add(keras.layers.Conv2D(filters=16, kernel_size=(1, 9), padding='same', strides=(1, 2)))\n",
        "    cnn.add(keras.layers.LeakyReLU(0.3))\n",
        "    cnn.add(keras.layers.BatchNormalization())\n",
        "    cnn.add(keras.layers.Dropout(0.3))\n",
        "\n",
        "    cnn.add(Symmetric('mean', axis=1))\n",
        "\n",
        "    cnn.add(keras.layers.Conv2D(filters=32, kernel_size=(1, 9), padding='same', strides=(1, 2)))\n",
        "    cnn.add(keras.layers.LeakyReLU(0.3))\n",
        "    cnn.add(keras.layers.BatchNormalization())\n",
        "    cnn.add(keras.layers.Dropout(0.3))\n",
        "\n",
        "    cnn.add(Symmetric('mean', axis=2))\n",
        "\n",
        "\n",
        "  elif model==3:\n",
        "    \"\"\"23 s/epoch\"\"\"\n",
        "\n",
        "    # None in input_shape for dimensions with variable size.\n",
        "    cnn.add(keras.layers.Conv2D(filters=8, kernel_size=(1, 9), padding='same', \n",
        "                                strides=(1, 1), input_shape=in_shape))\n",
        "    cnn.add(keras.layers.LeakyReLU(0.3))\n",
        "    cnn.add(keras.layers.BatchNormalization())\n",
        "\n",
        "    cnn.add(keras.layers.Conv2D(filters=16, kernel_size=(1, 9), padding='same', strides=(1, 2)))\n",
        "    cnn.add(keras.layers.LeakyReLU(0.3))\n",
        "    cnn.add(keras.layers.BatchNormalization())\n",
        "    cnn.add(keras.layers.Dropout(0.3))\n",
        "\n",
        "    cnn.add(Symmetric('mean', axis=1))\n",
        "\n",
        "    cnn.add(keras.layers.Conv2D(filters=16, kernel_size=(1, 9), padding='same', strides=(1, 2)))\n",
        "    cnn.add(keras.layers.LeakyReLU(0.3))\n",
        "    cnn.add(keras.layers.BatchNormalization())\n",
        "    cnn.add(keras.layers.Dropout(0.3))\n",
        "\n",
        "    cnn.add(keras.layers.Conv2D(filters=16, kernel_size=(1, 9), padding='same', strides=(1, 2)))\n",
        "    cnn.add(keras.layers.LeakyReLU(0.3))\n",
        "    cnn.add(keras.layers.BatchNormalization())\n",
        "    cnn.add(keras.layers.Dropout(0.3))\n",
        "\n",
        "    cnn.add(Symmetric('mean', axis=2))\n",
        "\n",
        "\n",
        "\n",
        "  elif model==4:\n",
        "    \"\"\"16 s/epoch\"\"\"\n",
        "\n",
        "    # None in input_shape for dimensions with variable size.\n",
        "    cnn.add(keras.layers.Conv2D(filters=16, kernel_size=(1, 9), padding='same', \n",
        "                                strides=(1, 2), input_shape=in_shape))\n",
        "    cnn.add(keras.layers.LeakyReLU(0.3))\n",
        "    cnn.add(keras.layers.BatchNormalization())\n",
        "\n",
        "    cnn.add(keras.layers.Conv2D(filters=16, kernel_size=(1, 9), padding='same', strides=(1, 2)))\n",
        "    cnn.add(keras.layers.LeakyReLU(0.3))\n",
        "    cnn.add(keras.layers.BatchNormalization())\n",
        "    cnn.add(keras.layers.Dropout(0.3))\n",
        "\n",
        "    cnn.add(Symmetric('mean', axis=1))\n",
        "\n",
        "    cnn.add(keras.layers.Conv2D(filters=32, kernel_size=(1, 9), padding='same', strides=(1, 2)))\n",
        "    cnn.add(keras.layers.LeakyReLU(0.3))\n",
        "    cnn.add(keras.layers.BatchNormalization())\n",
        "    cnn.add(keras.layers.Dropout(0.3))\n",
        "\n",
        "    cnn.add(Symmetric('mean', axis=2))\n",
        "\n",
        "\n",
        "\n",
        "  elif model==5:\n",
        "    \"\"\"12 s/epoch\"\"\"\n",
        "\n",
        "    # None in input_shape for dimensions with variable size.\n",
        "    cnn.add(keras.layers.Conv2D(filters=8, kernel_size=(1, 9), padding='same', \n",
        "                                strides=(1, 2), input_shape=in_shape))\n",
        "    cnn.add(keras.layers.LeakyReLU(0.3))\n",
        "    cnn.add(keras.layers.BatchNormalization())\n",
        "\n",
        "    cnn.add(keras.layers.Conv2D(filters=16, kernel_size=(1, 9), padding='same', strides=(1, 2)))\n",
        "    cnn.add(keras.layers.LeakyReLU(0.3))\n",
        "    cnn.add(keras.layers.BatchNormalization())\n",
        "    cnn.add(keras.layers.Dropout(0.3))\n",
        "\n",
        "    cnn.add(Symmetric('mean', axis=1))\n",
        "\n",
        "    cnn.add(keras.layers.Conv2D(filters=32, kernel_size=(1, 9), padding='same', strides=(1, 2)))\n",
        "    cnn.add(keras.layers.LeakyReLU(0.3))\n",
        "    cnn.add(keras.layers.BatchNormalization())\n",
        "    cnn.add(keras.layers.Dropout(0.3))\n",
        "\n",
        "    cnn.add(Symmetric('mean', axis=2))\n",
        "\n",
        "\n",
        "\n",
        "  elif model==6:\n",
        "    \"\"\"12 s/epoch\"\"\"\n",
        "\n",
        "    # None in input_shape for dimensions with variable size.\n",
        "    cnn.add(keras.layers.Conv2D(filters=128, kernel_size=(3, 3), activation='relu', \n",
        "                                input_shape=in_shape))\n",
        "    cnn.add(keras.layers.MaxPooling2D())\n",
        "\n",
        "    cnn.add(keras.layers.Conv2D(filters=128, kernel_size=(3, 3), activation='relu'))\n",
        "    cnn.add(keras.layers.MaxPooling2D())\n",
        "    cnn.add(keras.layers.Dropout(0.25))\n",
        "\n",
        "    cnn.add(keras.layers.Conv2D(filters=128, kernel_size=(3, 3), activation='relu'))\n",
        "    cnn.add(keras.layers.MaxPooling2D())\n",
        "    cnn.add(keras.layers.Dropout(0.25))\n",
        "\n",
        "    cnn.add(Symmetric('mean', axis=1))\n",
        "\n",
        "    cnn.add(keras.layers.Conv2D(filters=128, kernel_size=(1, 9), activation='relu'))\n",
        "    cnn.add(keras.layers.Dropout(0.25))\n",
        "\n",
        "    cnn.add(Symmetric('mean', axis=2))\n",
        "\n",
        "\n",
        "  elif model==7:\n",
        "    \"\"\"12 s/epoch\"\"\"\n",
        "\n",
        "    # None in input_shape for dimensions with variable size.\n",
        "    cnn.add(keras.layers.Conv2D(filters=128, kernel_size=(3, 3), activation='relu', \n",
        "                                input_shape=in_shape))\n",
        "    cnn.add(keras.layers.MaxPooling2D())\n",
        "\n",
        "    cnn.add(keras.layers.Conv2D(filters=128, kernel_size=(3, 3), activation='relu'))\n",
        "    cnn.add(keras.layers.MaxPooling2D())\n",
        "\n",
        "    cnn.add(keras.layers.Conv2D(filters=128, kernel_size=(3, 3), activation='relu'))\n",
        "    cnn.add(keras.layers.MaxPooling2D())\n",
        "\n",
        "    cnn.add(keras.layers.Conv2D(filters=128, kernel_size=(1, 9), activation='relu'))\n",
        "    cnn.add(keras.layers.MaxPooling2D())\n",
        "\n",
        "\n",
        "  elif model==8:\n",
        "    \"\"\"12 s/epoch\"\"\"\n",
        "\n",
        "    # None in input_shape for dimensions with variable size.\n",
        "    cnn.add(keras.layers.Conv2D(filters=8, kernel_size=(1, 5), padding='same', \n",
        "                                strides=(1, 2), input_shape=in_shape))\n",
        "    cnn.add(keras.layers.LeakyReLU(0.3))\n",
        "    cnn.add(keras.layers.BatchNormalization())\n",
        "\n",
        "    cnn.add(keras.layers.Conv2D(filters=16, kernel_size=(1, 5), padding='same', strides=(1, 2)))\n",
        "    cnn.add(keras.layers.LeakyReLU(0.3))\n",
        "    cnn.add(keras.layers.BatchNormalization())\n",
        "    cnn.add(keras.layers.Dropout(0.3))\n",
        "\n",
        "    cnn.add(Symmetric('mean', axis=1))\n",
        "\n",
        "    cnn.add(keras.layers.Conv2D(filters=32, kernel_size=(1, 5), padding='same', strides=(1, 2)))\n",
        "    cnn.add(keras.layers.LeakyReLU(0.3))\n",
        "    cnn.add(keras.layers.BatchNormalization())\n",
        "    cnn.add(keras.layers.Dropout(0.3))\n",
        "\n",
        "    cnn.add(Symmetric('mean', axis=2))\n",
        "\n",
        "\n",
        "  elif model==9:\n",
        "    \"\"\"12 s/epoch\"\"\"\n",
        "\n",
        "    # None in input_shape for dimensions with variable size.\n",
        "    cnn.add(keras.layers.Conv2D(filters=8, kernel_size=(1, 9), padding='same', \n",
        "                                strides=(1, 2), input_shape=in_shape))\n",
        "    cnn.add(keras.layers.LeakyReLU(0.3))\n",
        "    cnn.add(keras.layers.BatchNormalization())\n",
        "\n",
        "    cnn.add(keras.layers.Conv2D(filters=16, kernel_size=(1, 9), padding='same', strides=(1, 2)))\n",
        "    cnn.add(keras.layers.LeakyReLU(0.3))\n",
        "    cnn.add(keras.layers.BatchNormalization())\n",
        "    cnn.add(keras.layers.Dropout(0.5))\n",
        "\n",
        "    cnn.add(Symmetric('mean', axis=1))\n",
        "\n",
        "    cnn.add(keras.layers.Conv2D(filters=32, kernel_size=(1, 9), padding='same', strides=(1, 2)))\n",
        "    cnn.add(keras.layers.LeakyReLU(0.3))\n",
        "    cnn.add(keras.layers.BatchNormalization())\n",
        "    cnn.add(keras.layers.Dropout(0.5))\n",
        "\n",
        "    cnn.add(Symmetric('mean', axis=2))\n",
        "\n",
        "\n",
        "  elif model==10:\n",
        "    \"\"\"12 s/epoch\"\"\"\n",
        "\n",
        "    # None in input_shape for dimensions with variable size.\n",
        "    cnn.add(keras.layers.Conv2D(filters=8, kernel_size=(1, 5), padding='same', \n",
        "                                strides=(1, 2), input_shape=in_shape))\n",
        "    cnn.add(keras.layers.LeakyReLU(0.3))\n",
        "    cnn.add(keras.layers.BatchNormalization())\n",
        "\n",
        "    cnn.add(keras.layers.Conv2D(filters=16, kernel_size=(1, 5), padding='same', strides=(1, 2)))\n",
        "    cnn.add(keras.layers.LeakyReLU(0.3))\n",
        "    cnn.add(keras.layers.BatchNormalization())\n",
        "    cnn.add(keras.layers.Dropout(0.5))\n",
        "\n",
        "    cnn.add(Symmetric('mean', axis=1))\n",
        "\n",
        "    cnn.add(keras.layers.Conv2D(filters=32, kernel_size=(1, 5), padding='same', strides=(1, 2)))\n",
        "    cnn.add(keras.layers.LeakyReLU(0.3))\n",
        "    cnn.add(keras.layers.BatchNormalization())\n",
        "    cnn.add(keras.layers.Dropout(0.5))\n",
        "\n",
        "    cnn.add(Symmetric('mean', axis=2))\n",
        "\n",
        "\n",
        "  elif model==12:\n",
        "    \"\"\"12 s/epoch\"\"\"\n",
        "\n",
        "    # None in input_shape for dimensions with variable size.\n",
        "    cnn.add(keras.layers.Conv2D(filters=8, kernel_size=(1, 3), padding='same', \n",
        "                                strides=(1, 2), input_shape=in_shape))\n",
        "    cnn.add(keras.layers.LeakyReLU(0.3))\n",
        "    cnn.add(keras.layers.BatchNormalization())\n",
        "\n",
        "    cnn.add(keras.layers.Conv2D(filters=8, kernel_size=(1, 3), padding='same', strides=(1, 2)))\n",
        "    cnn.add(keras.layers.LeakyReLU(0.3))\n",
        "    cnn.add(keras.layers.BatchNormalization())\n",
        "    cnn.add(keras.layers.Dropout(0.5))\n",
        "\n",
        "    cnn.add(Symmetric('mean', axis=1))\n",
        "\n",
        "    cnn.add(keras.layers.Conv2D(filters=16, kernel_size=(1, 3), padding='same', strides=(1, 2)))\n",
        "    cnn.add(keras.layers.LeakyReLU(0.3))\n",
        "    cnn.add(keras.layers.BatchNormalization())\n",
        "    cnn.add(keras.layers.Dropout(0.5))\n",
        "\n",
        "    cnn.add(Symmetric('mean', axis=2))\n",
        "\n",
        "\n",
        "  elif model==13:\n",
        "    \"\"\"12 s/epoch\"\"\"\n",
        "\n",
        "    # None in input_shape for dimensions with variable size.\n",
        "    cnn.add(keras.layers.Conv2D(filters=8, kernel_size=(1, 5), padding='same', \n",
        "                                activation='relu', strides=(1, 2), input_shape=in_shape))\n",
        "    cnn.add(keras.layers.BatchNormalization())\n",
        "\n",
        "    cnn.add(keras.layers.Conv2D(filters=16, kernel_size=(1, 5), padding='same', \n",
        "                                activation='relu', strides=(1, 2)))\n",
        "    cnn.add(keras.layers.BatchNormalization())\n",
        "    cnn.add(keras.layers.Dropout(0.5))\n",
        "\n",
        "    cnn.add(Symmetric('mean', axis=1))\n",
        "\n",
        "    cnn.add(keras.layers.Conv2D(filters=32, kernel_size=(1, 5), padding='same', \n",
        "                                activation='relu', strides=(1, 2)))\n",
        "    cnn.add(keras.layers.BatchNormalization())\n",
        "    cnn.add(keras.layers.Dropout(0.5))\n",
        "\n",
        "    cnn.add(Symmetric('mean', axis=2))\n",
        "\n",
        "\n",
        "  cnn.add(keras.layers.Flatten())\n",
        "  cnn.add(keras.layers.Dense(1, activation='sigmoid'))\n",
        "  return cnn\n",
        "  \n",
        "\n",
        "\n",
        "def plot_average(x, y, param_name, name, bins=10):\n",
        "\n",
        "  x, y = np.array(x), np.array(y)\n",
        "  plotx = np.mean(x.reshape((-1, bins)), axis=1)\n",
        "  ploty = np.mean(y.reshape((-1, bins)), axis=1)\n",
        "  plt.plot(np.log10(plotx), ploty)\n",
        "  plt.title(name)\n",
        "  plt.ylabel('prediction D(x)')\n",
        "  plt.xlabel(param_name)\n",
        "  plt.ylim((0, 1))\n",
        "  plt.savefig(f'/content/results/{name}.png')\n",
        "  plt.clf()\n",
        "\n",
        "\n",
        "\n",
        "def generate_testdata(n_samples, seqlen, matrix_cols, param_name, low, high, n, minuslog):\n",
        "\n",
        "  print(f'generating {n} genotype matrices with different {param_name} from msprime for testing')\n",
        "  exps = -np.linspace(low, high, num=n) if minuslog else np.linspace(low, high, num=n)\n",
        "  testlist = np.power(10, exps)\n",
        "  x_test = simulate_msprime_list(n_samples, seqlen, matrix_cols, testlist, seed=None,\n",
        "                                 scale=True, param_name=param_name)\n",
        "\n",
        "  return x_test, testlist\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSdGkwmQq-Kx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This command allows for eager execution, where a static graph is compiled.\n",
        "# This allows the framework to apply global performance optimizations.\n",
        "# Applicable to any function that take tensors as input.\n",
        "#@tf.function \n",
        "def train_step(x, y):\n",
        "  \n",
        "  with tf.GradientTape() as tape:\n",
        "    # Run the forward pass of the layer. The operations that the layer applies\n",
        "    # to its inputs are going to be recorded on the GradientTape.\n",
        "    logits = discriminator(x, training=True)\n",
        "    loss_value = loss_fn(y, logits)\n",
        "  grads = tape.gradient(loss_value, discriminator.trainable_weights)\n",
        "  optimizer.apply_gradients(zip(grads, discriminator.trainable_weights))\n",
        "\n",
        "  train_acc_metric.update_state(y, logits)\n",
        "  return loss_value\n",
        "\n",
        "\n",
        "#@tf.function\n",
        "def test_step(x, y):\n",
        "    val_logits = discriminator(x, training=False)\n",
        "    val_acc_metric.update_state(y, val_logits)\n",
        "\n",
        "\n",
        "samples = 99              #Number of sampled haplotypes --> number of rows in the genotype matrix\n",
        "seqlen = 1e6              #Length of the randomly sampled genome region in bp\n",
        "genmat_cols = 128         #Number of columns of the genotype matrix after rescaling\n",
        "n_reps = 3000             #Number of repetitions --> number of genotype matrices\n",
        "epochs = 50               #Number of epochs for discriminator training\n",
        "batch_size = 32           #Size of each minibatch\n",
        "param_name = 'recomb'     #The parameter to test\n",
        "source = 'real data'      #Source of the data with the fixed parameters to infer\n",
        "fixed=1e-9                #The fixed value for the parameter in set A\n",
        "low=9                     #The lowest value for the randomly picked parameter in set B\n",
        "high=7                    #The highest value for the randomly picked parameter in set B\n",
        "\n",
        "\n",
        "# Prepare the training and validation datasets\n",
        "xtrain, xval, ytrain, yval = generate_data(samples, seqlen, n_reps, genmat_cols, param_name, \n",
        "                                  fixed, low, high, minuslog=True, scale=True, source=source)\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((xtrain, ytrain))\n",
        "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((xval, yval))\n",
        "val_dataset = val_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
        "\n",
        "# Prepare a list of genotype matrices from a range of values for the parameter for testing\n",
        "xtest, param_values = generate_testdata(samples, seqlen, genmat_cols, param_name, low, high, \n",
        "                                        n=10000, minuslog=True)\n",
        "\n",
        "print('Data simulation finished')\n",
        "\n",
        "# Prepare the metrics, optimizer and loss function\n",
        "train_acc_metric = keras.metrics.BinaryAccuracy()\n",
        "val_acc_metric = keras.metrics.BinaryAccuracy()\n",
        "optimizer = keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.5)\n",
        "loss_fn = keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "for model in [4, 7, 9, 13]:\n",
        "\n",
        "  img_paths = []\n",
        "\n",
        "  # Build the discriminator\n",
        "  print(f'Fitting discriminator model: {model}')\n",
        "  discriminator = build_discriminator(model, in_shape=(samples, genmat_cols, 1))\n",
        "\n",
        "  for epoch in range(1, epochs + 1):\n",
        "    \n",
        "    # Iterate over the batches of the dataset.\n",
        "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
        "      loss_value = train_step(x_batch_train, y_batch_train)\n",
        "\n",
        "    # Run a validation loop at the end of each epoch.\n",
        "    for x_batch_val, y_batch_val in val_dataset:\n",
        "      test_step(x_batch_val, y_batch_val)\n",
        "\n",
        "    # Display metrics at the end of each epoch.\n",
        "    train_acc = train_acc_metric.result()\n",
        "    val_acc = val_acc_metric.result()\n",
        "    print(f'Model {model} trained for {epoch} epochs')\n",
        "    print(\"Accuracy: Training - %.4f    Validation - %.4f\\n\" % (float(train_acc), float(val_acc)))\n",
        "    \n",
        "    # Reset training metrics at the end of each epoch\n",
        "    train_acc_metric.reset_states()\n",
        "    val_acc_metric.reset_states()\n",
        "\n",
        "    # Get discriminator prediction function over a range of values lin parameter space\n",
        "    predictions = discriminator.predict(xtest)\n",
        "    \n",
        "    # Plot the discriminator prediction function\n",
        "    name = f'D{model}test_{param_name}_{epoch}e_{low}:{high}'\n",
        "    plot_average(param_values, predictions, param_name, name, bins=100)\n",
        "    img_paths.append(f'/content/results/{name}.png')\n",
        "\n",
        "  # Save the sequence of images as a gif\n",
        "  images = [imageio.imread(filename) for filename in img_paths]\n",
        "  imageio.mimsave(f'D{model}_{param_name}_{source}.gif', images, format='GIF', fps=5)\n",
        "  keras.backend.clear_session()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPJeipjH2nFp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!zip -r /content/download.zip /content/D*\n",
        "files.download(\"/content/download.zip\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bKh4F74tCNy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -r /content/results/D*"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}